我們要開始這個禮拜的課程這個禮拜會到佼佼作業二然後宣布作業三我們今天的內容會是網路爬蟲也會是作業三的重點所以我會把今天的整個範例講完以後你們可以選擇你自己能力內可以寫到的程度因為寫程式其實有幾個階段一個階段是第二個階段是可以跑而且內容要正確第三個階段是可以跑內容要正確之後你還要能力模組化模組化的意思就是你可以把它建成function那這個模組化呢如果你要分享給更多不同的專案使用你還要能夠建成class就好像你去應用一個模組化你還要能夠建成class今天我就會針對這四個步驟來去講程式品質的四個階段那我喜歡上課模式都是融合性的就是我剛剛在描述的這四個階段可能很多時候會變成是一門叫做軟體工程的課然後它都是在產生在用軟體工程的方式然後你會看到你會看到你會看到你會看到你會看到你會看到然後它都是在產出寫程式的結構可是如果你沒有融入一個情境跟直接帶入一個實作你看完了你覺得好像在念一本如何管理程式的書可是其實你也不知道怎麼寫那你可以自己評估自己的實力來去把你的程式碼寫到多模組化能夠去分享給別人那上學期我是要求說他們一定要寫成模組化去分享給其他同學使用後來發現還蠻多同學做不到所以我今年就放寬就是你只要能夠寫出爬蟲但是我不限定你要做到模組化那因為我們班其實還蠻多臥虎藏龍的高手所以可能這樣子的要求對大家講太易了所以我還是維持如果你有能力把它做成模組化的話我在Github上就會看得到那等一下就會開始介紹一樣是爬蟲我從最簡單的Beautiful Soup等如何使用到變成最後變可以模組化成一個分享的class的樣子要怎麼寫那我們先來看一下大家的作業情況就是作業一已經都確認看完了那看完的話如果真的是差就是差了就是因為我已經宣導了好幾週都沒有跟我聯絡那沒有跟我聯絡也沒有停修然後名單一直都在那個交務處的系統上面其實我也不知道該怎麼辦到最後就是真的只能當掉所以如果有同學認識沒有交的同學的你可以幫忙提醒他一下在期末之前都還有一點機會但是如果我每一次錄影提醒提醒都沒有來跟我回應那你最後跟我說你要pass的話我就會拿錄影出來給你看說我每一週都有提醒可是你都沒有來找我這個禮拜的作業二我已經請助教開始做review所以那個review的結果應該下個禮拜可以看到那如果你對於作業有任何問題的話也都歡迎你們來找我因為像前幾天也都有同學來問我作業二的問題那你沒有來問我我就假設你沒有問題我不會主動去打擾你那我們今天要教的叫做Beautiful Super這個套件其實爬蟲的工具有超級多不是只有這一套那我教這一套算是簡單好上手那它也是有一些局限性的所以它只能抓比較簡單的網站那種比較複雜的類似像臉書它會擋一些爬蟲的行為或者是有一些你是要往下滑動它才會產生新的頁面出來的這種動態生成的或者是它的網站會有一些警示然後不讓你直接連續一直爬的這些可能Beautiful Super沒有辦法直接幫你解決你就要再去看一些進階的網頁開發的攻略然後才能知道要多寫些什麼東西去完成那種複雜網站的爬曲那爬蟲它是依網站而去寫的程式所以它不會是那種寫完一遍以後它就放出四行接准因為網頁的設計每一個網站都長不一樣除非那個網站幾十年來都沒有變有這樣的網站當然有PTT就是這樣幾十年來都沒有變所以我上課的範例一定都用PTT因為它的程式碼幾乎就不用改那如果說你是抓那種商業網站它為了防堵這些爬蟲程式一直干擾它的頻寬它會常常變動也會在裡面塞一些讓你爬起來很難改程式的按碼那這是正常因為這是個工坊它沒有允許你可以偷它的東西那爬蟲它本來就是在一個灰色地帶所以你要提取一個別人沒有Open Source給你的Data只是它公開在網站上秀出來它期望的是人真的去跟它互動而不是機器跟它互動所以爬蟲它並沒有嚴明規定說你不能爬但是你如果要爬它也會做一些防堵的行為所以爬蟲它本來就是一個沒有被很確認說它到底是合法還是非法的行為那所以這些商家它就會自己做一些保護措施所以你會說有些網站爬不下來那我就會說有可能它的防堵功率比你的程式功率高所以你爬不到就這麼簡單而已那BeautifulSoup它只是在建立你在提取Tag內的內容你比較有結構化的工具可以去提取方便你能夠在找到對的欄位的時候把資料抽出來有點像是Regular Expletion的那種感覺所以它沒有說什麼網站它都可以幫你爬到如果你要去爬其他網站你要解析其他網站複雜的那些按碼的時候那個你還要再去多學一些前端的概念所以我們今天不會帶那麼多所以我就是帶一些爬蟲的基礎知識好那爬蟲的流程是這樣子就是你一定會有一個對應的URL而且那個URL它是活著的如果你發現你URL送過去它Response回來是404就有兩種情況一個是你的網址根本就打錯了所以它找不到那個網站第二個就是那個網站關掉服務了所以如果你回應的Request不是200200叫做Connect Sussex那你就沒有辦法再往下去去寫程式你後面寫再多都沒有用所以第一件事情就是你要確定網路有通而且你能夠送出Request之後它Response給你的是200那你會問我說為什麼會有404 200這些編碼這不是我編的這是當初定義這些通訊協定的科學家大家講好的那你只要去查網路回應碼你就會知道有各種心態然後它有一個總表那這個總表就在這裡有1的、2的、3的、4的、5的類似像這樣有超級多種情況那我們現在要能夠順利爬衝我們就是要拿到200然後它會給你一個通訊協定然後你就可以去寫這個通訊協定那如果順利爬衝我們就是要拿到200那其他的編碼有其他你在互動過程中有些資安上的一些控制那基本上我們都不會做到那一段因為我們現在就是單純當一個瀏覽者只要單純當一個瀏覽者他就是在200狀態你眼睛看到網頁樣子就會是程式碼看到樣子那你有其他的這些Create、Assent這些其實是有更進階的伺服器端的互動今天爬衝都不會提到這些所以你要先知道爬衝它是有一個限制性的那拿到這個互動的請求而且成功連結上以後你就要開始做模擬換頁的行為那模擬換頁行為其實也只是在更動網址所以你會發現有些網址它很簡單就可以解析例如像PTT好了你在看PTT的時候你會發現它在瀏覽時如果我在這裡打這個編號然後這個編號其實會跟它的程式碼裡面這個編號可以找到呼應然後再來就是你換頁的時候你有沒有發現它在換頁的時候它有一個規律Index 39181然後3918039179就是它在產生網址換頁的時候它是一個流水號那所以你就很容易組織這個網址的合法性所以我們其實在做換頁的時候要嘛就是你從瀏覽的過程中網頁的蛛絲馬跡可以猜到它的下一頁網址長什麼樣子要嘛就是在你上下一頁的時候去看它的網址有沒有規律性所以這個是一般不想要做防堵的網站它會很佛心的就算隨便你它就程式好寫因為它這樣用個後回圈1 2 3 4 這樣計數下去它也好寫可是很多網站它不會這麼好心它可能會它可能會在換頁的時候它會把網址後面的請求跟你接的頁碼變成一個隨機的編號那只有它能夠去對應回去那你就沒有辦法反推那個編號到底怎麼來的那如果你又從網頁上沒有辦法找到你看這個網址變得很複雜如果又從網頁上你要找它的下一頁你的下一則假設說它這裡轉成這個然後你可能要從瀏覽這個內容以後再去找這個頁面裡可以找到分頁的例如可能找到這個那因為找到這個你可以從檢視原始碼看到那就是你現在可以從實務處裡面找到所有跟URL有關的你再去裡面細分抓它的分頁所以你要找到分頁你寫一個Followup其實只是假設我去看網址有什麼那你下一步就是你要能夠去檢視你的網址那你下一步就是你要能夠去檢視你的這一頁裡面其他的能不能被你從程式碼中提取出來那你一定要有這樣的能力要不然的話它不會自己腦補說你隨機的這個網址應該長怎樣其實我也不會知道所以這也是一種防堵的手法就是它在讓你瀏覽的時候不讓你算到那個分頁跟下一頁的位置然後就讓你沒有辦法拼湊出那個合理的網址所以我們今天為什麼可以用回圈訪問一頁一頁就是因為PTT它的網址很容易讓你算出那個流水號的範圍那個是它的程式碼提供了這樣子讓你很容易反推的現象不是說所有網站都有這樣子的這個佛心當我PTT能夠做到這樣子的換頁以後我就要開始解析我當頁拿到的因為PTT有兩層第一層是目錄第二層什麼點進目錄的某一篇文章裡面的內文所以我希望要找出當頁的目錄之外我又要把每一頁裡面提到的文章真的點進去再摳出來貼回Data Set裡面所以意思是什麼我目錄雖然只能看到標題作者時間可是我的Data Frame最後它可以對應到它的那個內容可以增加一個欄位貼上去那我先給大家看一下我做出來的樣子那我寫回的環境是把它寫回Google Sheet如果你不想寫在雲端上你也可以自己存到你的Local端為什麼我要多做這件事情因為我想說讓大家多一個活用雲端空間的能力所以等一下你就會看到說我的程式碼呢可以自動的去寫入一個Google Sheet然後你就可以直接把這個Google Sheet分享給其他人就是說我的Data Set已經抓好了所以這就是我抓到的所有內容那抓到的時候為什麼會這麼長是因為我在抓的時候我沒有去分說我是要抓貼文還是回文所以變成是它一點進去貼文回文一起回來那這個你也可以自己去做修改看你是要分析回文的還是只是看內容那Parton其實可以真的寫得非常複雜因為網站上的所有資料你要怎麼提取回來以後你必須去做分割或者是你要在在爬的當下就確定我要取哪一部分今天都會做Demo那你的作業就是憑你自己的能力來去做資料的收集與彙整那這件事情做到的話你第四次作業第五次作業你才有辦法做得更活潑或者是內容更豐富那最後輸出成Data Friend之後你才能夠存成CSE檔那為什麼一定要存下來如果你爬得那麼辛苦你不存下來你電腦關掉以後這Data Friend就消失了在你跑的當下跑完你電腦關機它已經從記憶體中釋放了你下次打開就是空的那你要幹嘛再重新爬一次所以你幹嘛浪費這些時間所以你一定要學會存檔所以爬蟲是一件事情爬蟲整理完存下來又是另外一件事情所以程式就是一環扣一環你要有能力把這些一環一環接起來那我現在要先來解釋Bluetooth的如何用Bluetooth的官方網站寫的非常簡單那我們 wynetooth 教大家怎麼閱讀它先假設你已經能夠拿 гo 這一坨東西了所以你有沒有看到它是用一個 Stream它是用一個 Stream 存一個假的網頁的內容它假設你拿下來了那它可以幹嘛它可以跟你說它能夠解析所有的Tag 有什麼解析出來以後你可以跟它說我要去 Pass哪一些 Tag例如我想要 Pass所有是 P 而且被Class 框住的那它就會幫你把這個有 Class Tag 的全部收集回來所以如果以同樣的道理我們來看一下 PTT你要先能夠觀察它有什麼 Tag 可以你使用 怎麼看呢假設我現在要看股票板好了我想要知道我拿的這個標題到底是在這個 BeautifulSoup 的程式裡面怎麼寫 你就按右鍵檢查然後你就會發現說它是被什麼包住它是被這個 DIV 的圖層包住而且它圖層的 Class 的名字叫 Title所以等一下你一定會用 BeautifulSoup去框出這個 Class 為 Title 的通通被你撈回來因為這裡面會有很多 Title所以它就會有這個 Title所以它就依序把所有的For All只要 Class 為 Title通通收集回來所以 BeautifulSoup 就很簡單如果你不用 BeautifulSoup你就真的要用 Regulus Preview 下去寫這一整坨裡面你要去框什麼可能是 A-H-R-E-F開頭然後可能到哪裡結尾然後這裡可能有 C-C-N-A 結尾你就用 Regulus Preview 去寫所以 BeautifulSoup 就是它幫你把 Tag 通通掃過一遍以後你可以用很簡潔的程式碼指定它要抓哪個 Tag 裡面的內容所以這就是 BeautifulSoup 主要的工作那我們現在再來看 BeautifulSoup的使用它就非常簡單你只要告訴它說你是要在 P-Tag 底下的Class所以你就跟它說 P-Tag 底下的 Class那如果舉一反三你想要抓的是這個你只要在 P-Tag 底下的 Class底下是 Title你就要寫 Class 等於 Title這樣就好了這樣你是不是就可以把這內容就掃回來了下一個動作就是因為我要知道怎麼樣把這個 Stream 掃回來就是 BeautifulSoup要先去跟前一個工具接叫做 RequestRequest 就是我可以透過 Python走瀏覽器的通道去跟它發送請求那它發送請求以後它就把整頁結果回吐給你那這樣子你就可以用 Stream 把它接回來接著用 BeautifulSoup 去做整理這樣了解了嗎所以整個邏輯就是這樣通常學完爬蟲的人他如果腦袋靈活一點他就聽得懂我在講的一些 Machine Learning API以及我講過的一些說可以去跟不同原件透過通訊協定走 CURL 通道都是同一件事情就是從 Python 發 Request所以寫程式它其實不是那種我學完這一課然後我就把它硬背下來你要知道它是可以上下延伸然後它可以串哪些工具好 那我們 BeautifulSoup 已經解釋過了我就真的要來實作囉那這個自訂類別的我可能會在 Clash 晚一點再講我們先直接來看我的程式碼我的程式碼就分成四大段第一段就是我講的能夠先執行但是我還沒有模組化那我們來講一下這個程式碼怎麼閱讀然後因為Google Sheet 它有那個空間的限制所以這個空間限制會導致我沒有辦法在這個同一個頁面繼續添加我的爬蟲內容因為它會說我的畫面已經塞滿了 我沒有辦法再貼上去所以我現在要增加一個新的分頁做測試然後等一下我會請它貼回來這邊程式碼是這樣子的你一定要有一個叫 Request 的工具因為它是要負責幫你跟網頁做互動的工具然後再來就是 BeautifulSoup然後再來就是你要把 BeautifulSoup 幫你解析完的結果存到 Pandas 裡面你才可以 Save 起來變成一個 Excel接下來我要去抓某一頁的文章那為什麼我寫成一個 Define 就是如果我寫成 Define 的話之後這一些內容我就可以不用管它我就這樣子我的程式碼就可以很簡潔 就只要寫這樣就好了那這個不就是你可以放在另外一個你的 ToolBase 底下的工具箱 然後去引入你自己寫的工具嗎那我們一段一段來因為你總要知道說這個 Define底下的內容是什麼你 RunDefine 的時候它什麼事都不會做它只是把你預備要有的程序記下來直到你呼叫它 它才執行這樣就是在 Define Function 的意思好 那我現在直接新增一段 我把這一段程式碼來做一個示範第一件事情是我是抓 PTT 的八卦版八卦版是不是有一個問你是不是有 18 歲所以你要模擬真的去把那個18 歲按我是 18 歲的行為塞給這個 Request 的時候一起送給瀏覽器然後去騙過它那這裡的 URL 呢就是我在這裡貼的八卦版那我現在先一段一段做 讓大家可以比較能夠跟得上我要先給它一個正確的網址然後這個網址呢是某一頁真的發生在網頁上能看得到的頁面好 那它已經讀進來了 讀進來以後呢我就要跟它說 我要用 Request GateGate 的意思就是我要提我現在看到這個網頁中的內容它看到的內容就會是這些它看到的不是這些 它看到是右邊這個所以 Request Gate 回來的是旁邊這邊的東西好 那這時候你就要跟它說Response 是來自於 Request這個 URL 之後並且騙過它得到的 那我們先來檢查一下那為什麼它不能做 因為我這邊Inport 還沒做好 那再 Run 一次那你想要知道它到底什麼回來 你就可以先請它把它 Show 給你看一下你要先確定你有成功的拿到真的是200 那你會說 我要怎麼看到 200 底下的內容所以接下來就是要用 Beautiful Super 去解析那這個解析裡面 因為這個Beautiful Super它要拿到的是一個 Stream所以這個 Response它其實是一個物件 裡面有一個成員變數叫 Text你只要能夠這樣點跑出項目可以選擇它就是一個物件因為它在裡面 它有非常多的角色有 Member Function 有 Member Variable就是對於這個物件底下的成員函式成員類別成員變數那為什麼你可以說它是一個物件因為它不是單純的數字文字只能存一種資料的形態而是它包含了功能以及記憶內容這樣子就叫物件那為什麼叫物件物件跟類別有什麼不一樣類別叫做定義這個未來物件被實體化以後的描述叫做類別當這個類別被實體化以後就叫物件 這樣有分的出來了嗎就是我定義一個人應該要怎麼樣怎麼樣就是要頭伸手腳叫做人 可是這個人不見得要被生出來當有一個新生兒被生出來那可能就是某某某的小孩他就是一個新的物件你也是一個人的類別下所存在的實體物件那當我確定這個物件裡面有一個變數是可以把所有他看到的內容掃回來的時候我確定一下真的我掃回來是這堆東西 我就可以邀請Beautiful Soup出來跟我互動了所以Beautiful Soup他在爬蟲裡面的角色並不是去抓網站 他其實是把網站上抓回來的東西做解析然後他用一個比較容易的解析手法可以讓你提取資料 如果你沒有用Beautiful Soup你要用Red Red Splash慢慢弄也可以 只是你的程式會寫超長好那我們來看一下我怎麼去寫這一段我的寫法是這樣因為我要先確定他是200我才要繼續往下走要不然的話我就會直接說這個網站沒有任何的回應所以重點是這裡Beautiful Soup裡面我確定要拿到是Stream 為什麼我知道這邊可以放進來是因為我這裡已經檢查過了而且我在Beautiful Soup的說明文件他是不是也跟你說你要怎麼用他 一定是Beautiful Soup呼叫放一個合法的Stream進來接著後面就是你要提取的Parser的條件 就這樣而已所以我們進來這邊之後 我跟他說什麼 我要Parser的是HTML底下的內容 而且幫我Parser完以後先幫我放在Soup裡面這樣OK嗎Parser就是我現在看到的是HTML那我想要去把這些Tag的位置都先定位出來 叫Parser就是取C的意思所以我就透過Beautiful Soup請他幫我把這一個拿回來的HTML把這些Tag的定位什麼叫Tag 就是這種大魚小魚的然後這樣子一坨的這樣就是TagTag代表是這個內容被什麼樣的屬性所包住那這個是HTML的另外一種觀念因為他在編輯網頁排版的時候他們都是依照這種一個一個Tag去表達接下來被你包住的內容要一樣子顯示給你他有什麼樣的連結功能字的形態 或者字的顏色字的大小 其實都是連結在這個Tag裡面你被包住的內容應該要怎麼顯示所以Parser其實就是去把這些Tag都盤點出來 你沒有先盤點你後面沒有辦法去下說我要直接跳到哪個Tag去這樣就是Beautyful Super在做的事情這裡有了解嗎好那當我確定這個Super有出來以後 我先走到這邊因為我要讓你們一段一段看Super裡面他解析出來的東西到底是什麼那我們這時候來看一下Super這個就是我一直跟你們說的如果你寫程式一坨程式裡面 你看不懂請你不要一坨就這樣RUN因為你還是不會知道他每一步在幹嘛因為我很熟了我當然可以趴趴趴這樣就跑過來那因為我後來發現這樣子的示範通訊們可能接受狀況有點差所以我這次就是改了一步一步做因為我本來是希望這一些動作是你們要自己做的我就要確定說這個Super到底回傳Parser玩的那些Tag有哪一些所以我就要硬硬看所以要怎麼做就是你每一次在做程式碼的時候走完一段就硬看看看他到底跑出來的東西跟你預期的內容有沒有一致 沒有就在這裡停下來不要急那你就會發現本來是這一坨長長的Stream結果被他整理成怎樣他真的盤點過了 這麼多的Tag都被看到了那就代表等一下這些Tag你都可以拿來強制指定他跳過去就這麼簡單所以這時候我先把它關掉要不然他太長了你就可以很開心的做什麼事情去做一個Full loop的要求為什麼是做一個Full loop的要求因為他的DIV ClassTitle不是只有一筆嗎Beauty and the Super他只有說你可以強制要求我跳到某個Tag可是這個Tag如果大於一個以上你要自己用Full loop幫我往前推進所以我要幹嘛我要說我要去尋找全部有DIV而且他的Class是Title的然後我要找全部File all他就會是一個List暫存所以我的Full loop要幹嘛我的Full loop就是說我的文章這個Article變數是自己命名的我的文章要在這個List裡面以後請你幫我依序的抽取出來所以這樣代表什麼代表你剛剛看到的這個依序拿出來的不是拿前面的Tag而是拿被Tag包住的文字因為Beauty and the Super是說我負責跳到這裡然後接下來再後回去裡面我跳到第一個我拿裡面的文字好所以我們再回來看程式那就一定會有些跟Tag有關的那個成員函式中的成員變數所以你就會發現說你看喔我要去拿什麼我要去拿Article這個後迴圈的暫時的變數裡面的Tag然後我去把它取出來那我們可以先試著做一個簡單的動作假設我去做一個File all然後我去拿一個Div這個Tag因為我剛剛已經看到Div裡面是有的然後Colossus我換另外一個好了因為希望你們可以舉1反3我們再來看一下有什麼如果這Colossus是一個Div然後你會看到你會看到它是一個Div如果這Colossus裡面有的是Search Bar好了那你看Search Bar裡面包的是什麼Search Bar裡面包的又是另外的Tag所以它等一下應該會印出一堆Tag底下的Tag所以最好你的Tag要挑到最下面那一層因為它是個槽狀挑到最下面那一層你才會一包進去它就是純粹的文字而不是中間又被其他的Tag所框住所以如果我現在沒有抓到一個非常好的Tag我倒的是這個Search Bar它中間包的不是直接接觸到內容你就會發現它會吐一大堆東西回來給你我請它幫我有沒有看到它就幫我把這個Tag底下包的東西全部都掃回來那掃回來以後我如果跟它說我要拿它的Tag然後它就跟你說它找不到因為它裡面都是Tag它沒有看到什麼Tag所以如果你發現這個問題的話你就要回去檢查你找的那個Tag是不是到最底下槽狀的內層然後它剛好跟著就真的是Tag的那個狀態好那你現在既然知道了我們再回到Claire讓一下然後因為它跟你說它這裡面一找出來應該會有很多不止一個然後它如果很多找出來不止一個的話那它就要是一個一個用Full圍圈去取出來所以我們現在可以先做幾個分段的動作如果我先請它印這個Tag看看會是什麼然後它就會有空白很大那空白很大是因為它是連同那個空格通通都掃進去了那這時候為什麼我要多這個就是我要把一些我不要的空白通通都幫我沾掉它就幫我黏在一起了所以如果你單純只是把Tag然後它就把空白通通都收回來那有可能網頁上一些你沒有注意到空白你也可以啟存進來那這樣瀏覽起來會在Data Set裡面就佔了一些空白的空間好那如果大家有了這個觀念之後我們下一步就是要抓某一頁的全部文章這個工作就做完了那我們要進步囉要進步是什麼可以抓換頁的文章換頁的文章它其實是有條件的因為PTT因為太容易觀察了我們怎麼觀察呢你回到這裡上頁上頁有沒有發現它的網址是什麼就是前面的這個內容然後再配Index再接流水號所以你每一天的日期配流水號是不一樣的所以你的那個數字如果要一日期抓的話實際上你是要做一些調整的因為你其實不知道編號在哪裡你要先檢查你的日期是不是在你的範圍內然後我再來去回頭看那怎麼看呢就是你要先看到到最新一定是IndexIndex之後你是不是可以看全文看全文以後你可以看這裡有日期然後再看這個上頁的檢查裡面可以找到這個號碼所以你會變得你程式會寫得很複雜你要幹嘛Index是一定不會動的可是你要先找到現在的當下的前一頁是幾號編號是幾號要從Parser裡面去找到這個BDHY的class然後去把這一段一條一直抽過來解析然後去做去你的段子把它斷掉然後去把這個數字取出來大家可以知道你現在往這個編號往前逆推幾頁所以這一段就比較複雜我就沒有示範了但是邏輯是這樣子寫的那如果你有興趣的話你也可以用我這樣的對話去問趣的QQ說請他幫你找在這一頁裡面是最新的頁面中的上一頁取出那一段的編號之後然後由這個編號往前推十頁那因為你很清楚這個邏輯你就可以去控制確認GVT生成你要的程式碼而不是說請你幫我抓什麼3月27號到幾月幾號他可能找出來程式碼你也不知道怎麼debug好那我現在為了要簡便我就直接不用日期因為日期要去做判別我就直接用號碼那我偷懶我就直接跟他說幾號到幾號是我確定存在號碼這樣子我就不用去檢查他存不存在可是有一個缺點是什麼這就代表沒有寫房貸如果隨便填了一個超過的那個數字那個網址是生不出來的他就會說找不到網頁那他就沒有辦法提醒說他的正確範圍在哪裡所以其實你不要小看房貸寫房貸是很麻煩的事情因為他要反推那個錯誤行為所以寫程式是這樣先能夠跑然後先假設操作都會是正確的情況下這只是第一步接下來會有一個QA就是用各種不合格行為來去驗證你的房貸寫得好不好那寫房貸的工程師就要去腦補可能會有哪些人做了什麼行為然後要去多寫可能讓你覺得功能沒有增加他只是在擋你但是那個擋的動作他寫的程式可能會超越那個原始功能的行數所以寫房貸其實真的蠻辛苦的好那我們就來解釋這段程式碼怎麼做因為我已經非常清楚他是靠這個index下面接這個頁碼來去生成的所以我就跟他說current page是我從startpage開始去做更新那這個start page跟end page是我想要從外部由使用者決定傳給我所以我就用一個方詢可以吃進兩個參數然後這兩個參數裡面這個start page跟end page我讓他跑一個迴圈去做一個取代因為如果我給他起點跟終點假設我要走1到10頁那我這個current page的start page其實會跟著這個迴圈一直去做置換的動作那我在做置換的動作我的response每一次的gate就要重新抓因為我的每一次的response其實是針對那個網址去做回應跟提取而不是說我建立連結以後我就完全不用管他下一頁的網址因為瀏覽器他就只能看網址內容所以你會發現他的換頁網址都跟著動所以每一個換頁只要網址移動你的request就要重新請求request是針對一個網址做反應所以你不要以為說我已經訪問過這個網站了為什麼要一直重新發request因為你只要換頁他request就換了所以request如果是curl的概念他其實是針對網址網址變你就要重新建立這個request所以為什麼在這邊會有新的request因為我的current page一直在動嘛那我一動了以後我的response都要檢查為什麼要檢查有的時候會有一些版主會把一些文章刪掉然後有些文章3333刪太多他那個頁面就自己消失所以那個就會跳號所以我一定要檢查我這一頁被我湊出來的網址存不存在我才要繼續往下勾引這裡的動作是不是都一模一樣跟我剛剛前面這裡寫的是不是一模一樣所以我只是多了幾個邏輯就是建立current page的那個取得編號的模式以及我每一次重建這個編號的頁碼的時候我就要重送request那你就會發現說這邊跟beauty or super都無關這邊其實是你在寫程式的邏輯存不存在那寫程式其實就是要有這種你可以自由的去替換這個邏輯去搭配不同的情境這種能夠想就能做的能力是要不斷反覆去寫的那最多都會是這種if else for因為它都是一個程序性的表達所以我就會確定說這個連結如果我要找上一頁下一頁的話我要一定的確保它存在就是因為我知道我在這個編號變動的時候假設我在這裡打1我在編號變動的時候這個下一頁從檢查裡面它可以去反查它是2是發生的是真的可以發生的所以這一段就是我在示範防呆這一段就算沒寫其實也沒差因為重點是在這個request已經做完了那我們就可以來試試看喔我現在想要從第一頁抓到第十頁所以我現在抓出來的都會是標題而已而且我都還沒有開始存到data set裡面那如果你很聰明你開始寫程式有感覺的話在哪裡我會用存到data set的append其實就是在所有的print位置把它改成什麼data friend append它就塞進去了因為印出來就代表你已經走到那一步了所以你把它印出來還是把它存起來就差別在這裡那我們再繼續講第三個寫成class的樣子寫成class的樣子就是我會把幾個大部分去同意定義例如說這個是initial setinitial set是什麼就是我還沒有執行程式但是它有一些初始資訊一定要知道的例如網址是什麼然後例如是你的有沒有要控制那個騙過他的按鈕over 18這樣子然後再來就是你要預備一個空的data friend來讓它存接下來等一下要存的資料所以這種是你還沒有開始做任何行為的時候你就要預備準備給它的定義跟儲存空間就會放在initial然後再來你這class裡面還有什麼因為我有訪問的是我們是不是從單頁到多頁所以你一定會想多頁其實是從for回圈裡面去摳單頁所以最小單位是什麼存單頁那我在這裡就定義我的單頁怎麼做那前面這邊呢是在做什麼也是做防呆所以這一段不寫其實也跟它從無關也沒關係可是你做了防呆你也是一個練習然後這裡是不是又一樣了有buttonsup然後parser去把剛剛單頁的那個peg都盤點出來那盤點出來以後現在教你們一個比較厲害的寫法叫try accepttry accept的意思就是說如果它真的跳頁了或者是它那個被版主刪掉的那個文章消失了它就真的找不到這些peg因為你的peg真的就不見了那我就會直接在run的時候不會報錯停下來而是系統會繼續keep going但是它會log你這個錯誤然後不會當掉就try accept意思就是說我讓系統有一個容錯的能力然後我在試這段內容如果能夠成功執行我就keep going如果我在試這段內容它錯了我就log出來說它錯在哪裡但是它下一個迴圈繼續往下走因為迴圈是這樣子你的邏輯是一路是對的但是是資料錯你只要跳過這筆資料你可以往下一筆資料繼續走但是如果你沒有寫try accept你fold迴圈就直接當在這裡對不對所以在寫卡沖或者是寫data science相關的工具的時候我們先確定的是可能八九成以上資料是對的我就讓它keep going這個邏輯然後我log什麼我log它拿幾筆資料出錯用try accept去把它攔下來但是不要擋住我的fold迴圈要不然的話你可能在第一底就出錯了後面九十九筆都是對的結果你的程式不會往下走這個是在邏輯放出資還接準這個範圍內的資料你是這樣處理是正確的時候你確定是正確的時候你就用try accept可是如果你的邏輯並不是這樣子可能這個fold迴圈跑完都會是完全一樣的邏輯的時候盡量不要這樣用因為我們這是方便你去檢查你用這個邏輯去反釋所有資料裡面有一些小的錯誤你就可以留下log好那當我defin完單頁的時候我就可以去defin多頁了那我這個defin多頁的時候我中間也是寫了一些防呆的工作那這個防呆的工作我寫完以後我就會請他去幫我把每一頁的內容點進去抽出來這是比較複雜一點為什麼會這樣說因為當我每一頁去拿到的時候我拿到titleosdata content可是content的裡面他點進去才看得到這樣點進去才看得到所以我是不是就要提取這個裡面的內容連結因為這個網址長不一樣這網址來自於哪裡是我上一頁這個title檢查被這個ahref這個tag包住的這裡面的網址我必須要留下來並且我在做內容提取的時候我要能夠再進去這一頁才能夠把這一頁裡面的內容全部再掃回來所以我在這裡要多做一件事情我是不是要去找這個ahref裡面的內容然後去跟他說我要去把這個裡面單頁內容抽出來所以我其實是提取了這個單頁內容以後請他幫我抓單頁這個單頁可以是目錄頁也可以是內容頁然後我讓他去抓這個內容頁因為我只要給他什麼我只要給他我確定的那個網址就好了我確定的網址是在title的時候我取出內容頁的網址給他他不就會幫我把內容頁取出來所以單頁除了可以取單頁的title之外你也可以把它想像成如果今天我的網址給的是內容頁我這個取單頁的功能其實也可以取到內容頁好所以在這裡你就會發現說我的這個article透過這個單頁的工具我就可以把它取回來取回來以後我就真的去把它save起來所以如果當你這個class定義的非常好你可以怎麼給別人用你就可以把它寫成一個module然後讓別人import那module其實很簡單你就是把它存成另外一個點拍檔然後它就變一個local端的點拍你在前面import這個點拍的名字就是這class的名字之後你就可以直接請他像這樣子呼叫你的class實體化這個物件之後就跟他說我要抓第幾頁到第幾頁並且把拿到的結果存起來所以他現在就真的在抓第一頁到第十頁因為第一頁到第十頁其實內容也算蠻多的所以他就一路抓抓完以後你再按這個df我們就可以來看看我是不是真的有抓到叫檢查第一頁的第一筆資料是那個2005年這個6月6月20號這個是1月不對這是6月6月20號星期一那我們來檢查一下其實你會寫爬蟲你就能做非常多事情了至少你不用用手動cobipass的搜集資料你就已經贏過超級多做苦工的人了那我們來看第一頁真的這是6月20號然後點進去他就拿到的資料我們來核對一下第一頁然後這個我全部拿到的時候不只是標題我還有content我要怎麼確定我的content存在我答錯嗎答錯了所以他就連同他的內容跟回文都一起被你掃回來了所以到這裡你的df就生成了後面這一段動作是我多加給你們的這個是for calllab如果你不是在calllab就不能這樣寫calllab前面這裡很簡單他就是要你授權他可以寫入你自己全線內的google sheet去跟他做連線所以這段程式碼其實不用什麼解釋因為這一坨都是在做授權檢查的行為然後我就真的去做授權就這樣而已那你貼回去他就會訪問你的google drive然後你就跟他說可以訪問然後他就自己跳轉到你要跟他連線的那個google帳號那因為calllab google drive他們都是google的附屬產品所以他的這個整個一連貫連線都是在google的環境下所以就照著他的授權要求這樣把pyson的程式碼貼上去你就可以直接跟你的google sheet連線那連線授權接下來就是要告訴他你要給他寫檔的動作那寫檔的動作他會有幾個條件是因為我要做防呆讓他不要超過我可以寫入的範圍那我在這一段你可以直接提用因為這一段就是我真的拿到這個授權的key我驗證了然後我也拿到了我的純入的dataset這個變數然後再來就是我會告訴這個df2sheet這個方式說我的這個key在哪裡然後我配到的這個id在哪裡id就這一串就是google sheet他每一個分頁都會有一個id那id他藏在網址有沒有看到他在這裡spread sheetd然後後面這一串講的就是他的id那你拿到這個id跟你的sheetnet是什麼?sheetnet就這個你拿到他就會知道要跑到哪一個分頁然後把檔案寫進去所以這時候呢我的寫檔工作就在這裡完成那我的id要給對喔就這邊這個id就是這個id然後我的這個sheetnet要改一下因為我剛剛已經改成test了然後這裡我就可以開始真的寫進去因為這裡只是define我還沒真的呼叫他我真的要寫進去他就開始一路寫寫寫有看到他是突然蹦就跑出來了這樣就寫出來了ok嗎?好那今天我已經把整個爬蟲的流程寫完了那因為為了不要讓你們一直來騷擾我的google sheet要不然就全班都跑來寫到我的檔案裡面所以我決定把我的id拿掉然後我等一下就會把我的id的寫入授權關掉我就不讓你們進來你就用你自己的所以你不能直接無腦的跑喔因為他就會說我找不到你要寫去哪裡好那play份程式碼在哪裡?這份程式碼在這邊所以其實今天我講的應該算是很完整所以請大家接下來的2個小時真的動手去試試看因為你不是你作業真的寫不出來那我的要求是什麼你不可以跟我抓一樣的東西然後再來你的程式碼不能完全跟我一樣因為我剛剛是不是有講過很多你還可以添加的房貸也可以啊或者是有其他的網站你放進來你一定要改那些beautifulsoup那個的寫法你不能完全跟我一樣網址跟我一樣我就完全不想看了因為你網址跟我一樣的時候其實你程式碼幾乎可以不用動了你只要動程式碼他會搭配你的網頁所以他們是換了抓的目標你的程式碼一定會開始動而且會強迫你們閱讀我給你們的範例一定要這麼做因為你如果沒有真的動手我還是我會的啊你還是不會啊那寫作業的目標是什麼就是希望你們會嘛好那我現在就把錄影關掉然後大家可以開始試那作業的要求就是這樣子自己選定一個要爬的網站然後就把網站爬下來然後這個網站爬到的內容呢你要用證者表達式清洗然後用beautifulsoup清洗那這個證者表達式清洗的意思是什麼因為裡面一定會有一些紙項目你拿到beautifulsoup只是tag底下的東西就像上次那個我demo的日文歌一樣我只是拿到歌詞可是我沒有辦法把整個真正我想要的那個日文段抽出來我還是用regular expression才抽的出來所以一定要用到證者表達式beautifulsoup你本來就會用了因為在爬蟲的階段你用tag的password你本來就會用到那因為我們上個禮拜教的是證者表達式所以你一定要把這個證者表達式融進去那我這份程式裡面沒有證者表達式所以代表這也是你們可以寫作業的空間啊因為如果全部都寫完了你們就沒有得想了然後再來因為我們有教到dick跟data friend所以我教你們存成data friend你要去想怎麼存成dick你可以比較簡單的做法就是data friendto json to dick他其實一行就轉過去了就像你從data friend to stack也是一行就轉過去了然後你感覺一下你資料結構怎麼樣去做互相控制然後再來你要存檔然後這一次因為比較複雜前面兩次因為程式碼太簡單了所以這次比較複雜請你們錄解說影片不是為了錄給我我會啊是錄給你們自己因為有可能你剛學你一陣子就會忘記了你錄下來其實是幫你之後如果忘記回來趕快看一下你當初幹什麼事接生你要從頭再去了解你當初想法的時間所以你錄youtube是幫自己的忙你要錄中文英文都可以但是我希望你們錄這個解說你幫自己進步以後如果有你的朋友想要學你很方便啊程式碼跟影片丟給他他不就會了嗎好那這就是第三次作業那第三次作業真的會比較難一點所以請你們現在就可以開始寫了開始試就會有問題有問題就可以來找我要不然你沒有試你就不會知道怎麼問問題
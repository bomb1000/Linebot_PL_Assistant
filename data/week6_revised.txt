我們要開始這個禮拜的課程，這個禮拜會到繳交作業二，然後宣布作業三，我們今天的內容會是網路爬蟲，也會是作業三的重點，所以我會把今天的整個範例講完以後，你們可以選擇你自己能力內，可以寫到的程度，因為寫程式其實有幾個階段，一個階段是可以跑，第二個階段是可以跑而且內容要正確，第三個階段是可以跑內容要正確之後，你還要能力模組化，模組化的意思就是，你可以把它建成function，那這個模組化呢，如果你要分享給更多不同的專案使用，你還要能夠建成class，就好像你去import別人的module，你就可以很容易的去建一個物件之後，直接呼叫那個物件裡面的function。今天我就會針對這四個步驟，來去講程式品質的四個階段，那我喜歡上課模式都是，融合性的，就是我剛剛，在描述的這四個階段，可能很多時候會變成是一門，叫做軟體工程的課，然後它都是在闡述寫程式的結構，可是如果你沒有融入一個情境，跟直接帶入一個實作，你看完了，你覺得好像在念一本如何管理程式的書，可是其實你也不知道怎麼寫，那你可以自己評估自己的實力，來去把你的程式碼寫到多模組化，能夠去分享給別人，那上學期我是要求說他們一定要寫成模組化去分享給其他同學使用，後來發現還蠻多同學做不到的，所以我今年就放寬，就是你只要能夠寫出爬蟲，但是我不限定你要做到模組化，那因為我們班其實還蠻多臥虎藏龍的高手，所以可能這樣子的要求對大家講太easy了，所以我還是維持，如果你有能力把它做成模組化的話，我在Github上就會看得到。那等一下就會開始介紹，一樣是爬蟲，我從最簡單的Beautiful Soup等如何使用，到變成最後變成可以模組化成一個分享的，class的樣子要怎麼寫，那我們先來看一下大家的作業情況，就是作業一呢，已經都確認，看完，看完的話，如果真的是差就是差了，就是因為我已經宣導了好幾週，都沒有跟我聯絡，那沒有跟我聯絡也沒有停修，然後名單一直都在那個教務處的系統上面，其實我也不知道該怎麼辦，到最後就是真的只能當掉，所以如果有同學認識沒有交的同學的，你可以幫忙提醒他一下，在期末之前都還有一點機會，但是如果我每一次錄影提醒提醒，然後都沒有來跟我回應，那你最後跟我說你要pass的話，我就會拿錄影出來給你看，說我每一週都有提醒啊，可是你都沒有來找我，好那這個禮拜的作業二呢，我已經請助教開始做review，所以那個review的結果應該下個禮拜可以看到，那如果你對於作業有任何問題的話，也都歡迎你們來找我，因為像前幾天也都有同學來問我作業二的問題，那你沒有來問我，我就假設你沒有問題，我不會主動去打擾你。好那我們今天要教的叫做Beautiful Soup這個套件，其實爬蟲的工具有超級多，不是只有這一套，那我教這一套算是簡單好上手，那它也是有一些侷限性的，所以它只能抓比較簡單的網站，那種比較複雜的，類似像臉書它會擋一些爬蟲的行為，或者是有一些你是要往下滑動，它才會產生新的頁面出來的，這種動態生成的，或者是它的網站會有一些警示，然後不讓你直接連續一直爬的，這些可能Beautiful Soup沒有辦法直接幫你解決，你就要再去看一些進階的網頁開發的攻略，然後才能知道要多寫些什麼東西，去完成那種複雜網站的爬取，那爬蟲它是依網站而去寫的程式，所以它不會是那種寫完一遍以後，它就放諸四海皆準，因為網頁的設計每一個網站都長不一樣，除非那個網站幾十年來都沒有變，那有這樣的網站當然有PTT就是這樣，幾十年來都沒有變，所以我上課的範例一定都用PTT，因為它的程式碼幾乎就不用改，那如果說你是抓那種商業網站，它為了防堵這些爬蟲程式，一直干擾它的頻寬，它會常常變動也會在裡面塞一些，讓你爬起來很難改程式的暗碼，那這是正常因為這是個攻防，它沒有允許你可以偷它的東西，那爬蟲它本來就是在一個灰色地帶，所以你要提取一個別人沒有Open Source給你的Data，只是它公開在網站上秀出來，它期望的是人真的去跟它互動，而不是機器跟它互動，所以爬蟲它並沒有嚴明規定說你不能爬，但是你如果要爬它也會做一些防堵的行為，所以爬蟲它本來就是一個沒有被很確認說，它到底是合法還是非法的行為，那所以這些商家它就會自己做一些保護措施，所以你會說有些網站爬不下來，那我就會說有可能它的防堵功率比你的程式功率高，所以你爬不到就這麼簡單而已，那Beautiful Soup它只是在建立你在提取tag內的內容，你比較有結構化的工具可以去提取，方便你能夠在找到對的欄位的時候把資料抽出來，有點像是Regular Expression的那種感覺，所以它沒有說什麼網站它都可以幫你爬到，如果你要去爬其他網站，你要解析其他網站複雜的那些暗碼的時候，那個你還要再去多學一些前端的概念，所以我們今天不會帶那麼多，所以我就是帶一些爬蟲的基礎知識。那爬蟲的流程是這樣子，就是你一定會有一個對應的URL，而且那個URL它是活著的，如果你發現你URL送過去，它Response回來是404就有兩種情況，一個是你的網址根本就打錯了，所以它找不到那個網站，第二個就是那個網站關掉服務了，所以如果你回應的Request不是200，200叫做Connect Success，那你就沒有辦法再往下去繼續寫程式，你後面寫再多都沒有用，所以第一件事情就是你要確定網路有通，而且你能夠送出Request之後，它Response給你的是200，那你會問我說為什麼會有404、200這些編碼，這不是我編的，這是當初定義這些通訊協定的科學家，大家講好的。那你只要去查網路回應碼，你就會知道有各種型態，然後它有一個總表，那這個總表就在這裡，有1的、2的、3的、4的、5的，類似像這樣，有超級多種情況，那我們現在要能夠順利爬蟲，我們就是要拿到200。那其他的編碼有其他你在互動過程中，有些資安上的一些控制，那基本上我們都不會做到那一段，因為我們現在就是單純當一個瀏覽者，只要單純當一個瀏覽者，它就是在200狀態，就會是你眼睛看到網頁樣子，就會是程式碼看到樣子。那你有其他的這些Create、Accept，這些其實是有更進階的伺服器端的互動，我們今天爬蟲都不會提到這些，所以你要先知道爬蟲它是有一個限制性的，那拿到這個互動的請求，而且成功連結上以後，你就要開始做模擬換頁的行為，那模擬換頁行為其實也只是在更動網址，所以你會發現有些網址它很簡單就可以解析。例如像PTT好了，你在看PTT的時候，你會發現它在瀏覽時如果我在這裡打這個編號，然後這個編號其實會跟，它的程式碼裡面這個編號可以找到呼應，然後再來就是你換頁的時候，你有沒有發現它在換頁的時候，它有一個規律，index 39181，然後39180，39179，就是它在產生網址換頁的時候，它是一個流水號，那所以你就很容易組織這個網址的合法性，所以我們其實在做換頁的時候，要嘛就是你從瀏覽的過程中，網頁的蛛絲馬跡可以猜到，它的下一頁網址長什麼樣子，要嘛就是在你上下一頁的時候，去看它的網址有沒有規律性，所以這個是一般不想要做防堵的網站，它會很佛心的就這樣隨便你，它就程式好寫，因為它這樣用個for迴圈，一二三四這樣計數下去它也好寫，可是很多網站它不會這麼好心，它可能會，在換頁的時候，它會把網址，後面的請求跟你接的頁碼，變成一個隨機的編號，那只有它能夠去對應回去，那你就沒有辦法反推那個編號到底怎麼來的，那如果你又從網頁上，沒有辦法找到，你看這個網址變得很複雜，如果又從網頁上你要找它的下一頁，你的下一則，假設說它這裡轉成這個，然後你可能要從，瀏覽這個內容以後，再去找，這個頁面裡可以找到分頁的，例如可能找到這個，那因為找到這個你可以從檢視原始碼看到，那你去裡面找到所有跟URL有關的，你再去裡面細分抓它的分頁，所以你要找到分頁，你寫一個for loop其實只是，說假設我去count網址流水號，那請下一步就是你要能夠去檢視，你的這一頁裡面，其他的能不能被你從，程式碼中提取出來，那你一定要有這樣的能力，要不然的話，它不會自己腦補說，你隨機的這個，網址應該長怎樣，其實我也不會知道，所以這也是一種防堵的手法，就是它在讓你瀏覽的時候，不讓你算到那個分頁跟下一頁的位置，然後就讓你沒有辦法拼湊出那個合理的網址，好所以我們今天為什麼可以用迴圈訪問一頁一頁，就是因為PTT它的網址，很容易讓你算出那個流水號的範圍，那個是它的程式碼提供了這樣子，讓你很容易反推的現象，不是說所有網站都有這樣子的這個佛心，好那當我PTT能夠做到這樣子的換頁以後，我就要開始解析我當頁拿到的，因為PTT有兩層，第一層是目錄，第二層什麼點進目錄的某一篇文章裡面的內文，所以我希望要找出當頁的目錄之外，我又要把每一頁裡面提到的文章真的點進去，再copy出來貼回Dataset裡面，所以意思是什麼，我目錄雖然只能看到標題作者時間，可是我的DataFrame最後它可以對應到它的那個內容，可以增加一個欄位貼上去，那我先給大家看一下我做出來的樣子。那我寫回的環境是把它寫回Google Sheet，那如果你不想寫在雲端上，你也可以自己存到你的Local端，那為什麼我要多做這件事情，因為我想說讓大家多一個活用雲端空間的能力，所以等一下你就會看到說，我的程式碼可以自動的去寫入一個Google Sheet，然後你就可以直接把這個Google Sheet分享給其他人，就會說我的Dataset已經抓好了，所以這就是我抓到的所有內容，那抓到的時候為什麼會這麼長，是因為我在抓的時候，我沒有去分說我是要抓貼文還是回文，所以變成是它一點進去，貼文回文一起回來，那這個你也可以自己去做修改，看你是要分析回文的，還是只是看內容，那爬蟲其實可以真的寫得非常複雜，因為網站上的所有資料，你要怎麼提取回來以後重新去做分割，或者是你要在爬的當下就確定我要取哪一部分，今天都會做Demo，那你的作業就是憑你自己的能力，來去做資料的收集與彙整，那這件事情做到的話，你第四次作業第五次作業，你才有辦法做得更活潑，或者是內容更豐富，那最後輸出成DataFrame之後，你才能夠存成CSV檔，那為什麼一定要存下來，如果你爬得那麼辛苦你不存下來，你電腦關掉以後，這DataFrame就消失了，在你跑的當下跑完，你電腦關機它已經從記憶體中釋放了，你下次打開它就是空的，那你要幹嘛，再重新爬一次，所以你幹嘛浪費這些時間，所以你一定要學會存檔，所以為什麼我要教你們存在Google Sheet，所以爬蟲是一件事情，爬蟲整理完存下來又是另外一件事情，所以寫程式它就是一環扣一環，你要有能力把這些一環一環接起來。好那我現在要先來解釋，Beautiful Soup怎麼用，Beautiful Soup的官方網站寫得非常簡單，那我們來教大家怎麼閱讀，它先假設你已經能夠拿到這一坨東西了，所以你有沒有看到它是用一個String，它是用String存一個假的網頁的內容，它假設你拿下來了，拿下來以後它可以幹嘛，它可以跟你說，它能夠解析所有的Tag有什麼，解析出來以後你可以跟它說，我要去Parser哪一些Tag，例如我想要Parser所有是P，而且被class框住的，那它就會幫你把這個，有class tag的全部收集回來，所以如果以同樣的道理，我們來看一下PTT，你要先能夠觀察它有什麼Tag可以使用，怎麼看呢，假設我現在要看股票板好了，我想要知道我拿的這個標題，到底是在這個Beautiful Soup的程式裡面怎麼寫，你就按右鍵檢查，然後你就會發現說，它是被什麼包住，它是被這個div的圖層包住，而且它圖層的class的名字叫title，所以等一下你一定會用Beautiful Soup，去框出這個class為title的通通被你撈回來，因為這裡面會有很多title，所以它就有這個title，所以它就依序把所有的for all，只要class為title的通通收集回來，所以Beautiful Soup就很簡單，那如果你不用Beautiful Soup的話，你就真的要用Regular Expression下去寫，是什麼就是這一整坨裡面，你要去框什麼，框可能是a href開頭，然後可能到哪裡結尾，然後這裡可能有/a結尾，你就用Regular Expression去寫，所以Beautiful Soup就是，它幫你把tag通通掃過一遍以後，你可以用很簡潔的程式碼，指定它要抓哪個tag裡面的內容，所以這就是Beautiful Soup主要的工作，那我們現在再來看Beautiful Soup的使用，它就非常簡單，你只要告訴它說，你是要在這個p tag底下的class，所以你就跟它說p tag底下的class，那如果舉一反三，你想要抓的是這個，不就是div這個tag底下的class，底下是title，你就要寫class等於title這樣就好了，這樣你是不是就可以把這內容就掃回來了，好，那下一個動作就是，因為我要知道怎麼樣把這個string掃回來，就是Beautiful Soup要先去跟前一個工具接，叫做request，request就是我可以透過Python走，瀏覽器的通道去跟它發送請求，那它發送請求以後，它就把整頁結果回吐給你，那這樣子你就可以用string把它接回來，接著用Beautiful Soup去做整理，這樣了解了嗎，所以整個邏輯就是這樣，所以通常學完爬蟲的人，他如果腦袋靈活一點，他就聽得懂我在講的一些Machine Learning API，以及我講過的一些，說可以去跟不同元件，透過通訊協定走CURL通道，都是同一件事情，就是從Python發request，所以寫程式它其實不是那種，我學完這一課，然後我就把它硬背下來，你要知道它是可以上下延伸，然後它可以串哪些工具，好 那我們Beautiful Soup已經解釋過了，我就真的要來實作了，那這個自訂類別的class晚一點再講，我們先直接來看我的程式碼。我的程式碼就分成四大段，第一段就是我講的能夠先執行，但是我還沒有模組化，那我們來講一下這個程式碼怎麼閱讀，然後因為Google Sheet它有那個空間的限制，所以這個空間限制，會導致我沒有辦法在這個同一個頁面，繼續添加新的爬蟲內容，因為它會說我的畫面已經塞滿了，我沒有辦法再貼上去，所以我現在要增加一個新的分頁做測試，然後等一下我會請它貼回來這邊，程式碼是這樣子的，你一定要有一個叫request的工具，因為它是要負責幫你跟網頁做互動的工具，然後再來就是Beautiful Soup，然後再來就是你要把Beautiful Soup，幫你解析完的結果存到Pandas裡面，你才可以save起來變成一個excel，接下來我要去抓某一頁的文章，那為什麼我寫成一個define，如果我寫成define的話，之後這一些內容我就可以不用管它，我就這樣子，我的程式碼就可以很簡潔，就只要寫這樣就好了，那這個不就是你可以放在另外一個，你的tool base底下的工具箱，然後去引入你自己寫的工具嘛，那我們一段一段來，因為你總要知道說這個define底下的內容是什麼，你run define的時候它什麼事都不會做，它只是把你預備要有的程序記下來，直到你呼叫它，它才執行，這樣就是在define function的意思，好那我現在直接新增一段，我把這一段程式碼來做一個示範，第一件事情是，我是抓PTT的八卦板，八卦板是不是有一個問你是不是有18歲，所以你要模擬真的去把那個18歲，按我是18歲的行為，塞給這個request的時候，一起送給瀏覽器騙過它，那這裡的URL呢，就是我在這裡貼的八卦板，那我現在先一段一段做，讓大家比較能夠跟得上。我要先給它一個正確的網址，然後這個網址呢，是某一頁真的發生在網頁上能看得到的頁面，好那它已經讀進來了，讀進來以後呢，我就要跟它說，我要用request get，get的意思就是我要提我現在看到這個網頁中的內容，它看到的內容就會是這些，它看到的不是這些，它看到的是右邊這個，所以request get回來的是旁邊這邊的東西，好那這時候你就要跟它說，response是來自於request這個URL之後，並且騙過它得到的，那我們先來檢查一下，那為什麼它不能做，因為我這邊import還沒做，好那再run一次，那你想要知道它到底什麼回來，你就可以先請它把response秀給你看一下，你要先確定你有成功的拿到，真的是200，那你會說我要怎麼看到200底下的內容，所以接下來就是要用Beautiful Soup去解析，那這個解析裡面，因為這個Beautiful Soup，它要拿到的是一個string，所以這個response它其實是一個物件，裡面有一個成員變數叫text，你只要能夠這樣點跑出項目可以選擇，它就是一個物件，因為它在裡面它有非常多的角色，有member function有member variable，就是對於這個物件底下的成員函式，成員類別，成員變數，那為什麼你可以說它是一個物件，因為它不是單純的數字文字，只能存一種資料的型態，而是它包含了功能以及記憶內容，這樣子就叫物件，那為什麼叫物件，物件跟類別有什麼不一樣，類別叫做定義這個未來物件被實體化以後的描述，叫做類別，當這個類別被實體化以後就叫物件，這樣有分的出來了嗎，就是我定義一個人應該要怎麼樣怎麼樣，就是要頭身手腳這樣子，叫做人，可是這個人不見得要被生出來，當有一個新生兒被生出來，那可能就是某某某的小孩，他就是一個新的物件，你也是一個人的類別下所存在的實體物件，那當我確定這個物件裡面有一個變數，是可以把所有他看到的內容掃回來的時候，我確定一下真的我掃回來是這堆東西，我就可以邀請Beautiful Soup出來跟我互動了，所以Beautiful Soup他在爬蟲裡面的角色並不是去抓網站，他其實是把網站上抓回來的東西做解析，然後他用一個比較容易的解析手法，可以讓你提取資料，如果你沒有用Beautiful Soup，你要用Regular Expression去慢慢弄，也可以,只是你的程式會寫超長，那我們來看一下我怎麼去寫這一段，我的寫法是這樣，因為我要先確定他是200，我才要繼續往下走，要不然的話我就會直接說這個網站沒有任何的回應，好,所以重點是這裡，Beautiful Soup裡面我確定要拿到的是String，為什麼我知道這邊可以放進來，是因為我這裡已經檢查過了，而且我在Beautiful Soup的說明文件，他是不是也跟你說，你要怎麼用他，你一定是Beautiful Soup呼叫，放一個合法的String進來，接著後面就是你要提取的Parser的條件，就這樣而已，所以我們進來這邊之後，我跟他說什麼，我要Parser的是HTML底下的內容，而且幫我Parser完以後先幫我放在Soup裡面，這樣OK嗎，那Parser是什麼，Parser就是我現在看到的是HTML，那我想要去把這些Tag的位置都先定位出來，叫Parser，Parser就是解析的意思，所以我就透過Beautiful Soup請他幫我把這一個，拿回來的HTML，把這些Tag的定位，什麼叫Tag，就是這種大於小於的，然後這樣子一坨的，這樣就是Tag，Tag代表是這個內容被什麼樣的屬性所包住，那這個是HTML的另外一種觀念，因為他在編輯網頁排版的時候，他們都是依照這種一個一個Tag去表達，接下來被你包住的內容，讓你樣子顯示給你，他有什麼樣的連結功能，字的型態，或者字的顏色，字的大小，其實都是連結在這個Tag裡面，告訴他，你被包住的內容應該要轉顯示，所以Parser其實就是去把這些Tag都盤點出來，你沒有先盤點，你後面沒有辦法去下說，我要直接跳到哪個Tag去，這樣就是Beautiful Soup在做的事情，這樣這裡有了解嗎，好，那當我確定這個Soup有出來以後，我先走到這邊，因為我要讓你們一段一段看Soup裡面，他解析出來的東西到底是什麼，我們這時候來看一下Soup，這個就是我一直跟你們說的，如果你寫程式，一坨程式裡面，你看不懂，請你不要一坨就這樣run，因為你還是不會知道他每一步在幹嘛，因為我很熟了，我當然可以趴趴趴這樣就跑過來，那因為我後來發現這樣子的示範，同學們可能那個接受狀況有點差，所以我這一次就是改的一步一步做，因為我本來是希望這一些動作是你們要自己做的，那我就要確定說這個Soup，到底回傳Parser完的那些Tag有哪一些嗎，所以我就要印印看，所以要怎麼做，就是你每一次在做程式碼的時候，走完一段就印看看，看他到底跑出來的東西跟你預期的內容，有沒有一致，沒有就在這裡停下來，不要急，那你就會發現，本來是這一坨長長的string，結果被他整理成怎樣，他真的盤點過了，有這麼多的Tag都被你看到了，那就代表等一下這些Tag，你都可以拿來強制指定他跳回去，就這麼簡單，好所以這時候我先把它關掉，要不然他太長了，你就可以很開心的做什麼事情，去做一個For loop的要求，為什麼是做一個For loop的要求，因為他的div Class Title不是只有一筆嗎，Beautiful Soup他只有說，你可以強制要求我跳到某個Tag，可是這個Tag如果大於一個以上，你要自己用For loop幫我往前推進，所以我要幹嘛呢，我要說我要去尋找全部有div，而且他的class是title的，然後我要找全部，Find all，那Find all之後他就會是一個list暫存，所以我的For loop要幹嘛，我的For loop就是說我的文章，這個article變數是自己命名的，我的文章要在這個list裡面以後，請你幫我依序的抽取出來，所以這樣代表什麼，代表你剛剛看到的，這個依序拿出來的不是拿前面的Tag，而是拿被Tag包住的文字，因為Beautiful Soup是說我負責跳到這裡，然後接下來在For迴圈裡面，我跳到第一個我拿裡面的文字，所以我們再回來看程式，那就一定會有些跟Text有關的，那個成員函式中的成員變數，所以你就會發現說，你看喔我要去拿什麼，我要去拿article這個，For迴圈的暫時的變數，裡面的Text，然後我去把它取出來，那我們可以先試著做一個簡單的動作，假設我去做一個Find all，然後我去拿一個div，這個Tag因為我剛剛已經看到DIV裡面是有的，然後Class，我換另外一個好了，因為希望你們可以舉一反三，我們再來看一下有什麼，如果這Class裡面有的是Search Bar好了，那你看Search Bar裡面包的是什麼，Search Bar裡面包的又是另外的Tag，所以它等一下應該會印出一堆Tag底下的Tag，所以最好你的Tag要挑到最下面那一層，因為它是個巢狀，挑到最下面那一層你才會一包進去，它就是純粹的文字，而不是中間又被其他的Tag所框住，所以如果我現在沒有抓到一個非常好的Tag，我找的是這個Search Bar，所以它中間包的不是直接接觸到內容，你就會發現它會吐一大堆東西回來給你，我請它幫我搜出來，有沒有看到，它就幫我把這個Tag底下包的東西全部都掃回來，那掃回來以後我如果跟它說我要拿它的Text，然後它就跟你說它找不到，因為它裡面都是Tag，它沒有看到什麼Text，所以如果你發現這個問題的話，你就要回去檢查，你找的那個Tag是不是到最底下巢狀的內層，然後它剛好跟著就真的是Text的那個狀態，好，那你現在既然知道了是這個樣子，我們再回到Class，再run一下，然後因為它跟你說，它這裡面一找出來應該會有很多不只一個，然後它如果很多找出來不只一個的話，那它就要是一個一個用For迴圈去取出來，所以我們現在可以先做幾個分段的動作，如果我先請它印這個Text，看看會是什麼，然後它就會有空白很大，那空白很大是因為它是連同那個空格都掃進去了，那這時候為什麼我要多這個，就是我要把一些我不要的空白通通都幫我刪掉，它就幫我黏在一起了，所以如果你單純只是把Text抽回來，那有可能網頁上一些你沒有注意到的空白，你也一起存進來，那這樣瀏覽起來會在Dataset裡面就佔了一些空白的空間，好那如果大家有了這個觀念之後，我們下一步就是要抓某一頁的全部文章，這個工作就做完了，那我們要進步了，要進步的是什麼，可以抓換頁的文章，那我剛剛有講換頁的文章它其實是有條件的，因為PTT因為太容易觀察了，我們怎麼觀察呢，你回到這裡上頁，有沒有發現它的網址是什麼，就是前面的這個內容，然後再配Index再接流水號，所以你每一天的日期配流水號是不一樣的，所以你的那個數字每天抓，如果要依日期抓的話，實際上你是要做一些調整的，因為你其實不知道編號在哪裡，你要先檢查你的日期是不是在你的範圍內，然後我再來去回頭看，那怎麼看呢，就是你要先看到到最新一定是Index，Index之後你是不是可以看全文，看全文以後你可以看這裡有日期，然後再看這個上頁的檢查，裡面可以找到這個號碼，所以你會變成你程式會寫得很複雜，你要幹嘛，你Index是一定不會動的，可是你要先找到現在的當下的前一頁是幾號，編號是幾號，要從Parser裡面去找到這個，bth wide的class，然後去把這一段一條一直抽出來解析，然後去做string的斷字把它斷掉，然後去把這個數字取出來，你才可以知道你現在往這個編號往前逆推幾頁，所以這一段就比較複雜，我就沒有示範了，但是邏輯是這樣子寫的，那如果你有興趣的話，你也可以用我這樣的對話去問去的Chat GPT說，請他幫你找在這一頁裡面是最新的，頁面中的上一頁取出那一段的編號之後，然後由這個編號往前推十頁，那因為你很清楚這個邏輯，你就可以去控制確的Chat GPT生成你要的程式，而不是說請你幫我抓什麼3月27號到幾月幾號，他可能找出來的程式，你也不知道怎麼debug，好那我現在為了要簡便，我就直接不用日期，因為日期要去做判別，我就直接用號碼，那我偷懶我就直接跟他說幾號到幾號，是我確定存在的號碼，這樣子我就不用去檢查他存不存在，可是有一個缺點是什麼，這就代表沒有寫防呆，如果隨便填了一個超過的數字，那個網址是生不出來的，他就會說找不到網頁，那他就沒有辦法提醒說他的正確範圍在哪裡，所以其實你不要小看防呆，寫防呆是很麻煩的事情，因為他要反推那個錯誤行為，所以寫程式是這樣，先能夠跑，然後先假設操作都會是正確的情況下，這只是第一步，接下來會有一個QA，就是用各種不合格行為，來去驗證你的防呆寫得好不好，那寫防呆的工程師就要去腦補，可能會有哪些人做了什麼行為，然後要去多寫，可能讓你覺得功能沒有增加，他只是在擋你，但是那個擋的動作，他寫的程式可能會超越那個原始功能的行數，所以寫防呆其實真的蠻辛苦的，那我們就來解釋這段程式怎麼做，因為我已經非常清楚，他是靠這個index下面，接這個頁碼來去生成的，所以我就跟他說，current page是我從start page開始去做更新，那這個start page跟end page，是我想要從外部，由使用者決定傳給我，所以我就用一個function可以吃進兩個參數，然後這兩個參數裡面，start page跟end page，我讓他跑一個for迴圈去做一個取代，因為如果我給他起點跟終點，假設我要走1到10頁，那我這個current page的start page，其實會跟著這個for迴圈，一直去做置換的動作，那我在做置換的動作，我的response每一次的get就要重新抓，因為我的每一次的response，其實是針對那個網址去做回應跟提取，而不是說我建立連結以後，我就完全不用管他下一頁的網址，因為瀏覽器他就只能看網址內容，所以你會發現他的換頁，網址都跟著動，所以每一個換頁只要網址一動，你的request就要重新請求，request是針對一個網址做反應，所以你不要以為說，我已經訪問過這個網站了，為什麼要一直重新發request，因為你只要換頁他request就換了，所以request如果是CURL的概念，他其實是針對網址，網址變你就要重新建立這個request，所以為什麼在這邊會有新的request，因為我的current page一直在動，那我一動了以後，我的response都要檢查，為什麼要檢查，因為PTT有的時候會有一些版主，會把一些文章刪掉，然後有些文章刪太多，他那個頁面就會自己消失，所以那個就會跳號，所以我一定要檢查我這一頁，被我湊出來的網址存不存在，我才要繼續往下going，這裡的動作是不是都一模一樣，跟我剛剛前面這裡寫的，是不是一模一樣，所以我只是多了幾個邏輯，就是建立current page的取得編號的模式，以及我每一次重建這個編號的頁碼的時候，我就要重送request，你就會發現說，這邊跟Beautiful Soup都無關，這邊其實是你在寫程式的邏輯存不存在，寫程式其實就是要有這種，你可以自由的去替換這個邏輯，去搭配不同的情境，這種能夠想就能做的能力，是要不斷反覆去寫的，最多都會是這種if else for，因為它都是一個程序性的表達，所以我就會確定說，這個連結如果我要找上頁下頁的話，我要一定的確保它存在，就是因為我知道我在這個編號變動的時候，假設我在這裡打1，我在編號變動的時候，這個下頁從檢查裡面，它可以去反查它是2，是發生的，是真的可以發生的，所以這一段就是我在示範防呆，這一段就算沒寫其實也沒差，因為重點是在這個request已經做完了，那我們就可以來試試看，我現在想要從第一頁抓到第十頁，所以我現在抓出來的都會是標題而已，而且我都還沒有開始存到dataset裡面，那如果你很聰明，你開始寫程式有感覺的話，在哪裡我會用存到dataset的append，其實就是在所有的print位置把它改成什麼，DataFrame append，它就塞進去了，因為印出來就代表你已經走到那一步了，只是你要把它印出來還是把它存起來，就差別在這裡，那我們再繼續講第三個，寫成class的樣子，寫成class的樣子就是我會把幾個大部分去統一定義，例如說這個是initial set，initial set是什麼，就是我還沒有執行程式，但是它有一些初始資訊一定要知道的，例如網址是什麼，然後例如是你的session有沒有要控制那個騙過它的按鈕，over 18這樣子，然後再來就是，你要預備一個空的DataFrame，來讓它存接下來等一下要存的資料，所以這種是你還沒有開始做任何行為的時候，你就要預備準備給它的定義跟儲存空間，就會放在initial，然後再來你這class裡面還有什麼，因為我有訪問的是單頁，我們是不是從單頁到多頁，所以你一定會想，多頁其實是從for迴圈裡面去call單頁，所以最小單位是什麼，存單頁，那我在這裡就定義我的單頁怎麼做，前面這邊是在做什麼，也是做防呆，所以這一段不寫其實也跟它從無關，也沒關係可是你做了防呆，你也是一個練習，然後這裡是不是又一樣了，有Beautiful Soup，然後Parser去把剛剛單頁的，那個tag都盤點出來，那盤點出來以後現在教你們一個，比較厲害的寫法叫try except，try except的意思就是說，如果它真的跳頁了，或者是它那個，被版主刪掉的那個文章消失了，它就真的找不到這些tag，因為這些tag真的就不見了，那我就會，直接在run的時候不會報錯停下來，而是系統會繼續keep going，但是它會log你這個錯誤，然後不會down掉，就try except意思就是說，我讓系統有一個容錯的能力，然後我在試這段內容，如果能夠成功執行我就keep going，如果我在試這段內容它錯了，我就log出來說它錯在哪裡，但是它下一個迴圈繼續往下走，因為迴圈是這樣子，你的邏輯是一路是對的，但是資料錯，你只要跳過這筆資料，你可以往下一筆資料繼續走嗎，但是如果你沒有寫try except，你for迴圈就直接當在這裡，所以在寫爬蟲或者是寫data science相關的工具的時候，我們先確定的是可能，八九成以上的資料是對的，我就讓它keep going這個邏輯，然後我log什麼，我log它拿幾筆資料出錯，用try except去把它，攔下來但是不要擋住我的for迴圈，要不然的話你可能在第一筆就出錯了，後面九十九筆都是對的，結果你的程式不會往下走，這個是在邏輯，放諸四海皆準這個範圍內的資料，你是這樣處理是正確的時候，你確定是正確的時候你就用try except，可是如果你的邏輯並不是這樣子，可能這個for迴圈跑完都不會是完全一樣的邏輯的時候盡量不要這樣用，因為我們這是方便你去檢查，你用這個邏輯去，仿勢所有資料裡面，有一些小的錯誤，你就可以留下log，好，當我define完單頁的時候，我就可以去define多頁了，我define多頁的時候中間也是寫了一些，防呆的工作，那這個防呆的工作，我寫完以後，我就會請他去幫我，把每一頁的內容，點進去抽出來，這個是比較複雜一點，為什麼會這樣說，因為當我每一頁去拿到的時候，我拿到title、author、date、content，可是這個content的裡面，他要點進去才看得到，這樣點進去才看得到，所以我是不是就要提取，這個裡面的內容連結，因為這個網址長不一樣，這個網址來自於哪裡，是我上一頁，這個title，檢查被這個ahref，這個tag包住了這裡面的，網址我必須要留下來，並且我在做內容提取的時候，我要能夠再進去這一頁，才能夠把，這一頁裡面的，內容全部再掃回來，所以我在這裡，又多做一件事情，你看我是不是要去找，這個href裡面的內容，然後去跟他說，我要去把這個，裡面單頁內容，抽出來，所以我其實是提取了，這個單頁內容以後請他幫我抓單頁，這個單頁可以是目錄頁，也可以是內容頁，然後我讓他去抓這個內容頁，因為我只要給他什麼，我確定的那個網址就好了，那我確定的網址是在title的時候，我取出內容頁的網址給他，他不就會幫我把內容頁取出來，所以單頁除了可以取單頁的title之外，你也可以把它想像成，如果今天我的網址給的是內容頁，我這個取單頁的功能其實也可以取到內容頁。好，那所以在這裡，你就會發現說，我的這個article透過這個，單頁的工具我就可以，把它取回來，取回來以後呢，我就真的去把它save起來，所以如果當你這個class，定義的非常好，你可以怎麼給別人用，你就可以把它寫成一個module，然後讓別人import，那module其實很簡單，你就是把它存成另外一個.py檔，然後它就變成一個local端的.py，那你在前面import這個.py的名字，就是這class的名字，之後呢，你就可以直接，請他像這樣子，呼叫你的class，實體化這個物件之後，就跟他說我要抓第幾頁到第幾頁，然後並且把拿到的結果存起來，所以他現在就真的，在抓第一頁到第十頁，因為第一頁到第十頁，其實內容也算蠻多的，所以他就一路抓，好抓完以後你再按這個df，好我們就可以來看看，我是不是真的有抓到，叫檢查嘛，第一頁的第一筆資料，是那個2005年，這個，6月，是嗎，6月20號，這個是1月，不對這是6月，6月20號星期一，好那我們來檢查一下，其實你會寫爬蟲，你就能做非常多事情了，至少你不用手動copy paste的搜集資料，你就已經贏過超級多做苦工的人了，好那我們來看第一頁，真的這是6月20號，然後點進去，然後他就拿到的資料，我們來核對一下，第一頁，gogoegg，這個，然後我，全部拿到的時候不只是標題，我還有content，我要怎麼確定我的content存在，我打錯嗎，哦打錯，所以他就連同他的內容跟回文，都一起被你掃回來了，所以到這裡你的，df就生成了，那生成之後後面這一段動作，是我多加給你們的，這個是for colab，如果你不是在colab就不能這樣寫，那colab前面這裡很簡單，他就是要你授權他可以寫入，你自己權限內的google sheet，那去跟他做連線，所以這段程式碼其實不用什麼解釋，因為這一坨，都是在做授權檢查的行為，然後我就去做授權就這樣而已，那你貼回去他就會，訪問你的google drive，然後你就跟他說可以訪問，然後他就自己跳轉到，你要跟他連線的那個google帳號，那因為colab、google drive他們都是google的附屬產品，所以他的整個一連貫連線都是在google的環境下，所以就照著他的授權要求，這樣把Python的程式碼貼上去，你就可以直接跟你的google sheet連線，那連線授權接下來就是要告訴他，你要給他寫檔的動作，那寫檔的動作，他會有，幾個條件是因為我要做防呆，讓他不要，超過我可以寫入的範圍，那我在這一段，你可以直接提用，因為這一段，就是我真的拿到這個授權的key，我驗證了然後我也拿到了我的存入的Dataset這個變數，然後再來就是我會告訴這個dftosheet這個function說，我的這個key在哪裡然後我配到的這個id在哪裡，id就這一串，就是google sheet他每一個分頁都會有一個id，那id他藏在網址有沒有看到他在這裡，spreadsheet d，然後後面這一串講的就是他的id，那你拿到這個id，跟你的sheet name，sheet name是什麼sheet name就是這個，你拿到他就會知道要跑到，哪一個分頁然後把，檔案寫進去，所以這時候呢，我的這個寫檔工作就在這裡完成，那我的id要給對喔，我就這邊，這個id，就是這個id然後我的這個sheet name，要改一下因為我剛剛已經改成test，然後這裡，我就可以開始真的寫進去，因為這裡只是define我還沒真的呼叫他，我真的要寫進去他就開始一路寫寫寫寫，有看到他是突然蹦就跑出來了，這樣就寫出來了OK嗎，好那今天我已經把整個，爬蟲的流程寫完了，那因為為了不要讓你們一直來，騷擾我的google sheet要不然就全班，都跑來又寫到我的檔案裡面，所以我決定把我的id拿掉，然後我等一下就會把我的，id的寫入授權關掉，我就不讓你們進來，你就用你自己的，所以你不能直接無腦的，跑喔因為他就會說我找不到，你要寫去哪裡，好那這一份程式碼在哪裡，這份程式碼在這邊，所以其實今天我講的應該算是很完整，所以請大家接下來的兩個小時真的動手去試試看，因為你不試你作業真的寫不出來，那我的要求是什麼，你不可以跟我抓一樣的東西，然後再來你的程式碼不能完全跟我一樣，因為我剛剛是不是有講過，很多你還可以添加的防呆也可以，或者是有其他的網站你放進來，你一定要改那些Beautiful Soup的tag的寫法，你不能完全跟我一樣，網址跟我一樣我就完全不想看了，因為你網址跟我一樣的時候，其實你程式碼幾乎可以不用動了，你只要動程式碼，它會搭配你的網頁，所以他們是換了抓的目標，你的程式碼一定會開始動，而且會強迫你們閱讀我給你們的範例，一定要這麼做因為你如果沒有真的動手，我還是我會的你還是不會啊，那寫作業的目標是什麼，就是希望你們會。好，那我現在就把錄影關掉，然後大家可以開始試，那作業的要求就是這樣子，自己選定一個要爬的網站，然後就把網站爬下來，然後這個網站爬到的內容，你要用正則表達式清洗，然後用Beautiful Soup清洗，那這個正則表達式清洗的意思是什麼，因為裡面一定會有一些子項目，你拿到Beautiful Soup只是tag底下的東西，就像上次那個我demo的日文歌一樣，我只是拿到歌詞，可是我沒有辦法把整個真正我想要的那個日文段抽出來，我還是用regular expression才抽的出來嘛，所以一定要用到正則表達式，Beautiful Soup你本來就會用了，因為在爬蟲的階段你用tag的parser你本來就會用到，那因為我們上個禮拜教的是正則表達式，所以你一定要把這個正則表達式融進去，那我這份程式裡面沒有正則表達式，所以代表這也是你們可以寫作業的空間啊，因為如果全部都寫完了你們就沒有得想了，然後再來因為我們有教到dict跟dataframe，所以我教你們存成dataframe你要去想怎麼存成dict，你可以比較簡單的做法就是dataframe to json，to dict它其實一行就轉，過去了就像你從dataframe to stack也是一行就轉過去了，只是要讓你們感覺一下，你資料結構怎麼樣去做互相控制，然後再來你要存檔，然後這一次因為比較複雜，前面兩次因為程式碼太簡單了，所以這次比較複雜請你們錄解說影片，不是為了錄給我，我會啊，是錄給你們自己，因為有可能你剛學，你一陣子就會忘記了，你錄下來，其實是幫你之後如果忘記回來趕快看一下你當初幹的什麼事，節省你要從頭再去瞭解你當初的想法的時間，所以你錄youtube是幫自己的忙，你要錄中文英文都可以，但是我希望你們錄這個解說你幫自己進步，以後如果有你的朋友想要學，你很方便啊，程式碼跟影片丟給他他不就會了嗎，好那這就是第三次作業，那第三次作業真的會比較難一點，所以可以請你現在就可以開始寫了，你開始試就會有問題，有問題就可以來找我，要不然你沒有試你就不會知道怎麼問問題